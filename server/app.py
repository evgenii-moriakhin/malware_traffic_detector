import asyncio
import logging
import os
import pickle
from functools import partial
from aiohttp import web
from aiofiles import open as aio_open
from nfstream import NFStreamer
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sqlalchemy import create_engine


DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise KeyboardInterrupt("Not env var DATABASE_URL!")
engine = create_engine(DATABASE_URL)

# Configure the logger
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


dataset_path = "/app/datasets/Syn.csv"
model_file_path = "/app/models/Syn_random_forest_model.pkl"

columns_to_keep = [
        "Destination Port",
        "Flow Duration",
        "Total Fwd Packets",
        "Total Backward Packets",
        "Total Length of Fwd Packets",
        "Total Length of Bwd Packets",
        "Flow Bytes/s",
        "Flow Packets/s",
        "Flow IAT Mean",
        "Flow IAT Max",
        "Flow IAT Min",
        "Fwd IAT Total",
        "Fwd IAT Mean",
        "Fwd IAT Max",
        "Fwd IAT Min",
        "Bwd IAT Total",
        "Bwd IAT Mean",
        "Bwd IAT Max",
        "Bwd IAT Min",
        "SYN Flag Count",
        "CWE Flag Count",
        "ECE Flag Count",
        "URG Flag Count",
        "ACK Flag Count",
        "PSH Flag Count",
        "RST Flag Count",
        "FIN Flag Count",
        "Label"
    ]


if os.path.exists(model_file_path):
    # Load the saved model from the file
    with open(model_file_path, "rb") as model_file:
        model = pickle.load(model_file)
else:
    logger.info("Read dataset csv")
    dataset = pd.read_csv(dataset_path)
    logger.info("Success read dataset csv")
    # Remove spaces from column names
    dataset.columns = [column.strip() for column in dataset.columns]

    # Filter the dataset to keep only the subset of features
    dataset = dataset[columns_to_keep]

    dataset.fillna(dataset.mean(), inplace=True)

    # Replace infinity values with the maximum finite value for float32
    dataset.replace([np.inf, -np.inf], np.finfo(np.float32).max, inplace=True)

    X = dataset.drop("Label", axis=1)
    y = dataset["Label"]

    logger.info("Train Split")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model
    logger.info("Training...")
    model = RandomForestClassifier(
        n_estimators=500,
        max_depth=None,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features="auto",
        criterion="gini",
        bootstrap=True,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    logger.info(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    logger.info(classification_report(y_test, y_pred))

    # Save the trained model to a file
    with open(model_file_path, "wb") as model_file:
        pickle.dump(model, model_file)


# Create a partial function for NFStreamer
nfstream_create = partial(
    NFStreamer,
    source="to_analyze.pcap",
    decode_tunnels=True,
    bpf_filter=None,
    promiscuous_mode=True,
    snapshot_length=1536,
    idle_timeout=120,
    active_timeout=1800,
    accounting_mode=1,
    udps=None,
    n_dissections=20,
    statistical_analysis=True,
    splt_analysis=0,
    n_meters=0,
    performance_report=0,
)


async def capture_pcap(request):
    """Функция "capture_pcap" используется для захвата сетевого трафика и сохранения его в файл формата pcap. Она принимает запрос HTTP и считывает данные из этого запроса.

    Если данные считаны успешно, функция открывает файл "packets.pcap" в режиме добавления (append mode) с помощью модуля "aio_open" и записывает считанные данные в файл. Затем файл "packets.pcap" сохраняется, и функция возвращает HTTP-ответ с текстом "success".

    Если данные не были считаны успешно, функция возвращает HTTP-ответ с текстом "no data".
    """
    data = await request.read()
    if data:
        async with aio_open("packets.pcap", "ab") as out:
            await out.write(data)
            await out.flush()
        return web.Response(text="success")
    else:
        return web.Response(text="no data")


async def analyze_pcap():
    """Функция "analyze_pcap" предназначена для анализа файла с сетевым трафиком в формате pcap. Она работает в бесконечном цикле и каждые 30 секунд проверяет наличие файла "packets.pcap" в текущей директории. Если файл существует, он переименовывается в "to_analyze.pcap", после чего создается объект NFStreamer, который используется для анализа трафика и преобразования его в Pandas DataFrame.

    Количество потоков в объекте NFStreamer подсчитывается и выводится в лог. Затем DataFrame, полученный из объекта NFStreamer, обрабатывается с помощью функции "process_nfstream_df", после чего применяется модель машинного обучения "model" для предсказания значений. Предсказанные значения сохраняются в DataFrame в новом столбце "prediction".

    Далее данные из DataFrame сохраняются в базу данных SQL с помощью функции "to_sql", которая добавляет новые строки в таблицу "packages". В конце цикла файл "to_analyze.pcap" удаляется, и процесс начинается заново.

    Функция может выкидывать исключения, которые будут отловлены и продолжится выполнение функции. Вся информация о процессе работы функции сохраняется в лог с помощью модуля "logger".
    """
    while True:
        try:
            if os.path.exists("packets.pcap"):
                logger.info(f"packets.pcap exists. Rename to to_analyze.pcap")
                os.rename("packets.pcap", "to_analyze.pcap")
                logger.info("Creating NFStreamer")
                package_streamer = nfstream_create()
                logger.info("NFStreamer created")

                # Print the number of flows in the NFStreamer object
                flow_count = sum(1 for _ in package_streamer)
                logger.info(f"Number of flows: {flow_count}")

                logger.info("Converting streamer to pandas DataFrame")
                package_df = package_streamer.to_pandas()
                logger.info("Streamer successfully converted to DataFrame")
                logger.info("DataFrame columns:")
                logger.info(package_df.columns)
                processed_df = process_nfstream_df(package_df)
                logger.info(f"Prediction...")
                predictions = model.predict(processed_df)
                logger.info(f"Success prediction")
                package_df["prediction"] = predictions
                package_df.to_sql("packages", con=engine, if_exists="append")
                try:
                    logger.info(f"remove to_analyse.pcap")
                    os.remove("to_analyse.pcap")
                except Exception as e:
                    logger.warning(f"error {repr(e)} while remove to_analyse.pcap")
                else:
                    logger.info(f"removed to_analyse.pcap")
        except Exception as e:
            logger.info(f"Error {e} in analyse loop. Continue...")
            continue
        await asyncio.sleep(30)


def process_nfstream_df(df):
    # Select and rename relevant features from the NFStreamer DataFrame
    processed_df = df[
        [
            "dst_port",
            "bidirectional_duration_ms",
            "src2dst_packets",
            "dst2src_packets",
            "src2dst_bytes",
            "dst2src_bytes",
            "src2dst_first_seen_ms",
            "src2dst_last_seen_ms",
            "dst2src_first_seen_ms",
            "dst2src_last_seen_ms",
            "src2dst_min_ps",
            "src2dst_mean_ps",
            "src2dst_stddev_ps",
            "src2dst_max_ps",
            "dst2src_min_ps",
            "dst2src_mean_ps",
            "dst2src_stddev_ps",
            "dst2src_max_ps",
            "src2dst_min_piat_ms",
            "src2dst_mean_piat_ms",
            "src2dst_stddev_piat_ms",
            "src2dst_max_piat_ms",
            "dst2src_min_piat_ms",
            "dst2src_mean_piat_ms",
            "dst2src_stddev_piat_ms",
            "dst2src_max_piat_ms",
            "src2dst_syn_packets",
            "src2dst_cwr_packets",
            "src2dst_ece_packets",
            "src2dst_urg_packets",
            "src2dst_ack_packets",
            "src2dst_psh_packets",
            "src2dst_rst_packets",
            "src2dst_fin_packets",
            "dst2src_syn_packets",
            "dst2src_cwr_packets",
            "dst2src_ece_packets",
            "dst2src_urg_packets",
            "dst2src_ack_packets",
            "dst2src_psh_packets",
            "dst2src_rst_packets",
            "dst2src_fin_packets",
        ]
    ].rename(
        columns={
            "dst_port": "Destination Port",
            "bidirectional_duration_ms": "Flow Duration",
            "src2dst_packets": "Total Fwd Packets",
            "dst2src_packets": "Total Backward Packets",
            "src2dst_bytes": "Total Length of Fwd Packets",
            "dst2src_bytes": "Total Length of Bwd Packets",
        }
    )

    # Calculate additional features
    processed_df["Flow Bytes/s"] = (processed_df["Total Length of Fwd Packets"] + processed_df["Total Length of Bwd Packets"]) / processed_df[
        "Flow Duration"]
    processed_df["Flow Packets/s"] = (processed_df["Total Fwd Packets"] + processed_df["Total Backward Packets"]) / processed_df[
        "Flow Duration"]

    processed_df["Flow IAT Mean"] = processed_df[["src2dst_mean_piat_ms", "dst2src_mean_piat_ms"]].mean(axis=1)
    processed_df["Flow IAT Max"] = processed_df[["src2dst_max_piat_ms", "dst2src_max_piat_ms"]].max(axis=1)
    processed_df["Flow IAT Min"] = processed_df[["src2dst_min_piat_ms", "dst2src_min_piat_ms"]].min(axis=1)

    processed_df["Fwd IAT Total"] = processed_df["Total Fwd Packets"] * processed_df["src2dst_mean_piat_ms"]
    processed_df["Fwd IAT Mean"] = processed_df["src2dst_mean_piat_ms"]
    processed_df["Fwd IAT Max"] = processed_df["src2dst_max_piat_ms"]
    processed_df["Fwd IAT Min"] = processed_df["src2dst_min_piat_ms"]

    processed_df["Bwd IAT Total"] = processed_df["Total Backward Packets"] * processed_df["dst2src_mean_piat_ms"]
    processed_df["Bwd IAT Mean"] = processed_df["dst2src_mean_piat_ms"]
    processed_df["Bwd IAT Max"] = processed_df["dst2src_max_piat_ms"]
    processed_df["Bwd IAT Min"] = processed_df["dst2src_min_piat_ms"]

    # TCP flags related features
    processed_df["SYN Flag Count"] = processed_df["src2dst_syn_packets"] + processed_df["dst2src_syn_packets"]
    processed_df["CWE Flag Count"] = processed_df["src2dst_cwr_packets"] + processed_df["dst2src_cwr_packets"]
    processed_df["ECE Flag Count"] = processed_df["src2dst_ece_packets"] + processed_df["dst2src_ece_packets"]
    processed_df["URG Flag Count"] = processed_df["src2dst_urg_packets"] + processed_df["dst2src_urg_packets"]
    processed_df["ACK Flag Count"] = processed_df["src2dst_ack_packets"] + processed_df["dst2src_ack_packets"]
    processed_df["PSH Flag Count"] = processed_df["src2dst_psh_packets"] + processed_df["dst2src_psh_packets"]
    processed_df["RST Flag Count"] = processed_df["src2dst_rst_packets"] + processed_df["dst2src_rst_packets"]
    processed_df["FIN Flag Count"] = processed_df["src2dst_fin_packets"] + processed_df["dst2src_fin_packets"]

    # Fill missing values with the mean of the respective column
    processed_df.fillna(processed_df.mean(), inplace=True)

    # Replace infinity values with the maximum finite value for float32
    processed_df.replace([np.inf, -np.inf], np.finfo(np.float32).max, inplace=True)

    # Create a new DataFrame with only the selected columns
    selected_cols = list(columns_to_keep)[:-1]  # Label col must be last
    processed_df_selected = processed_df[selected_cols]

    return processed_df_selected


""" Код представляет собой главную программу для запуска сервера для анализа сетевого трафика.

Функция "background_tasks" создает задачу "analyse_task" с помощью функции "create_task", которая запускает функцию "analyze_pcap" для анализа сохраненного файла с сетевым трафиком. Затем функция "yield" передает управление главной программе, чтобы она могла продолжить работу. В конце функция "await" ждет завершения задачи "analyse_task".

Функция "main" создает объект приложения "app" с помощью класса "web.Application" и добавляет маршрут "/" для обработки запросов HTTP с помощью функции "capture_pcap". Затем с помощью метода "cleanup_ctx.append" добавляется функция "background_tasks" в контекст очистки (cleanup context), чтобы она запускалась при завершении работы сервера.

В конце, если этот модуль был запущен как основной (через условную конструкцию "if name == 'main':"), функция "main" запускается с помощью метода "web.run_app", который запускает сервер в бесконечном цикле до тех пор, пока его не остановят.
"""


async def background_tasks(app):
    analyse_task = asyncio.create_task(analyze_pcap())
    yield
    await analyse_task


def main():
    app = web.Application(client_max_size=None)
    app.add_routes([web.post("/", capture_pcap)])
    app.cleanup_ctx.append(background_tasks)
    web.run_app(app)


if __name__ == "__main__":
    main()
