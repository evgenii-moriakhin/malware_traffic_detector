import asyncio
import logging
import os
import pickle
from functools import partial
from aiohttp import web
from aiofiles import open as aio_open
from nfstream import NFStreamer
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sqlalchemy import create_engine


DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise KeyboardInterrupt("Not env var DATABASE_URL!")
engine = create_engine(DATABASE_URL)

# Configure the logger
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Prepare a dataset with features and labels for training the model.
# Replace this with your dataset of choice.

model_file_path = "/app/models/random_forest_model.pkl"


if os.path.exists(model_file_path):
    # Load the saved model from the file
    with open(model_file_path, "rb") as model_file:
        model = pickle.load(model_file)
else:
    logger.info("Read dataset csv")
    dataset = pd.read_csv("/app/datasets/Wednesday-workingHours.pcap_ISCX.csv")
    logger.info("Success read dataset csv")

    # Filter the dataset to keep only the subset of features
    columns_to_keep = [
        " Destination Port",
        " Flow Duration",
        " Total Fwd Packets",
        " Total Backward Packets",
        "Total Length of Fwd Packets",
        " Total Length of Bwd Packets",
        "Flow Bytes/s",
        " Flow Packets/s",
        " Flow IAT Mean",
        " Flow IAT Max",
        " Flow IAT Min",
        "Fwd IAT Total",
        " Fwd IAT Mean",
        "Bwd IAT Total",
        " Bwd IAT Mean",
        " Label",
    ]
    dataset = dataset[columns_to_keep]

    # Fill missing values with the mean of the respective column
    dataset.fillna(dataset.mean(), inplace=True)

    # Replace infinity values with the maximum finite value for float32
    dataset.replace([np.inf, -np.inf], np.finfo(np.float32).max, inplace=True)

    X = dataset.drop(' Label', axis=1)
    y = dataset[' Label']

    logger.info("Train Split")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model
    logger.info("Training...")
    model = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=1,
                                   max_features='auto', criterion='gini', bootstrap=True, n_jobs=-1)
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    logger.info(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    logger.info(classification_report(y_test, y_pred))

    # Save the trained model to a file
    with open(model_file_path, "wb") as model_file:
        pickle.dump(model, model_file)


# Create a partial function for NFStreamer
nfstream_create = partial(
    NFStreamer,
    source="to_analyze.pcap",
    decode_tunnels=True,
    bpf_filter=None,
    promiscuous_mode=True,
    snapshot_length=1536,
    idle_timeout=120,
    active_timeout=1800,
    accounting_mode=1,
    udps=None,
    n_dissections=20,
    statistical_analysis=False,
    splt_analysis=0,
    n_meters=0,
    performance_report=0,
)


async def capture_pcap(request):
    data = await request.read()
    if data:
        async with aio_open("packets.pcap", "ab") as out:
            await out.write(data)
            await out.flush()
        return web.Response(text="success")
    else:
        return web.Response(text="no data")


async def analyze_pcap():
    while True:
        try:
            if os.path.exists("packets.pcap"):
                logger.info(f"packets.pcap exists. Rename to to_analyze.pcap")
                os.rename("packets.pcap", "to_analyze.pcap")
                logger.info("Creating NFStreamer")
                package_streamer = nfstream_create()
                logger.info("NFStreamer created")

                # Print the number of flows in the NFStreamer object
                flow_count = sum(1 for _ in package_streamer)
                logger.info(f"Number of flows: {flow_count}")

                logger.info("Converting streamer to pandas DataFrame")
                package_df = package_streamer.to_pandas()
                logger.info("Streamer successfully converted to DataFrame")
                logger.info("DataFrame columns:")
                logger.info(package_df.columns)
                processed_df = process_nfstream_df(package_df)
                logger.info(f"Prediction...")
                predictions = model.predict(processed_df)
                logger.info(f"Success prediction")
                package_df['prediction'] = predictions
                inserted_num = package_df.to_sql("packages", con=engine, if_exists="append")
                logger.info(f"Amount of inserted and analyzed rows to DB is {inserted_num}")
                try:
                    logger.info(f"remove to_analyse.pcap")
                    os.remove("to_analyse.pcap")
                except Exception as e:
                    logger.warning(f"error {repr(e)} while remove to_analyse.pcap")
                else:
                    logger.info(f"removed to_analyse.pcap")
        except Exception as e:
            logger.info(f"Error {e} in analyse loop. Continue...")
            continue
        await asyncio.sleep(30)


def process_nfstream_df(df):
    # Select and rename relevant features from the NFStreamer DataFrame
    processed_df = df[['dst_port', 'bidirectional_duration_ms', 'src2dst_packets', 'dst2src_packets',
                       'src2dst_bytes', 'dst2src_bytes', 'src2dst_first_seen_ms', 'src2dst_last_seen_ms',
                       'dst2src_first_seen_ms', 'dst2src_last_seen_ms']].rename(columns={
        'dst_port': ' Destination Port',
        'bidirectional_duration_ms': ' Flow Duration',
        'src2dst_packets': ' Total Fwd Packets',
        'dst2src_packets': ' Total Backward Packets',
        'src2dst_bytes': 'Total Length of Fwd Packets',
        'dst2src_bytes': ' Total Length of Bwd Packets',
    })

    # Calculate additional features
    processed_df['Flow Bytes/s'] = (processed_df['Total Length of Fwd Packets'] +
                                     processed_df[' Total Length of Bwd Packets']) / processed_df[' Flow Duration']
    processed_df[' Flow Packets/s'] = (processed_df[' Total Fwd Packets'] +
                                       processed_df[' Total Backward Packets']) / processed_df[' Flow Duration']

    # Calculate Flow IAT features
    processed_df[' Flow IAT Mean'] = (processed_df['src2dst_last_seen_ms'] - processed_df['src2dst_first_seen_ms']) / (processed_df[' Total Fwd Packets'] - 1)
    processed_df[' Flow IAT Max'] = processed_df['src2dst_last_seen_ms'] - processed_df['src2dst_first_seen_ms']
    processed_df[' Flow IAT Min'] = 0  # Since we don't have per-packet timestamps, we can't accurately calculate this value

    # Calculate Fwd IAT features
    processed_df['Fwd IAT Total'] = processed_df['src2dst_last_seen_ms'] - processed_df['src2dst_first_seen_ms']
    processed_df[' Fwd IAT Mean'] = processed_df[' Flow IAT Mean']

    # Calculate Bwd IAT features
    processed_df['Bwd IAT Total'] = processed_df['dst2src_last_seen_ms'] - processed_df['dst2src_first_seen_ms']
    processed_df[' Bwd IAT Mean'] = (processed_df['dst2src_last_seen_ms'] - processed_df['dst2src_first_seen_ms']) / (processed_df[' Total Backward Packets'] - 1)

    # Remove old NFStreamer columns
    processed_df.drop(columns=['src2dst_first_seen_ms', 'src2dst_last_seen_ms',
                               'dst2src_first_seen_ms', 'dst2src_last_seen_ms'], inplace=True)

    # Fill missing values with the mean of the respective column
    processed_df.fillna(processed_df.mean(), inplace=True)

    # Replace infinity values with the maximum finite value for float32
    processed_df.replace([np.inf, -np.inf], np.finfo(np.float32).max, inplace=True)

    return processed_df


async def background_tasks(app):
    analyse_task = asyncio.create_task(analyze_pcap())
    yield
    await analyse_task


def main():
    app = web.Application(client_max_size=None)
    app.add_routes([web.post("/", capture_pcap)])
    app.cleanup_ctx.append(background_tasks)
    web.run_app(app)


if __name__ == "__main__":
    main()
