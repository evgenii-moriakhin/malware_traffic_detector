import asyncio
import logging
import os
import pickle
from functools import partial
from aiohttp import web
from aiofiles import open as aio_open
from nfstream import NFStreamer
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sqlalchemy import create_engine


DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise KeyboardInterrupt("Not env var DATABASE_URL!")
engine = create_engine(DATABASE_URL)

# Configure the logger
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


model_file_path = "/app/models/random_forest_model.pkl"


if os.path.exists(model_file_path):
    # Load the saved model from the file
    with open(model_file_path, "rb") as model_file:
        model = pickle.load(model_file)
else:
    logger.info("Read dataset csv")
    dataset = pd.read_csv("/app/datasets/Syn.csv")
    logger.info("Success read dataset csv")

    # Filter the dataset to keep only the subset of features
    columns_to_keep = [
        " Destination Port",
        " Flow Duration",
        " Total Fwd Packets",
        " Total Backward Packets",
        "Total Length of Fwd Packets",
        " Total Length of Bwd Packets",
        "Flow Bytes/s",
        " Flow Packets/s",
        " Flow IAT Mean",
        " Flow IAT Max",
        " Flow IAT Min",
        "Fwd IAT Total",
        " Fwd IAT Mean",
        "Bwd IAT Total",
        " Bwd IAT Mean",
        " Label",
    ]
    dataset = dataset[columns_to_keep]

    dataset.fillna(dataset.mean(), inplace=True)

    # Replace infinity values with the maximum finite value for float32
    dataset.replace([np.inf, -np.inf], np.finfo(np.float32).max, inplace=True)

    X = dataset.drop(" Label", axis=1)
    y = dataset[" Label"]

    logger.info("Train Split")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model
    logger.info("Training...")
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features="auto",
        criterion="gini",
        bootstrap=True,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    logger.info(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    logger.info(classification_report(y_test, y_pred))

    # Save the trained model to a file
    with open(model_file_path, "wb") as model_file:
        pickle.dump(model, model_file)


# Create a partial function for NFStreamer
nfstream_create = partial(
    NFStreamer,
    source="to_analyze.pcap",
    decode_tunnels=True,
    bpf_filter=None,
    promiscuous_mode=True,
    snapshot_length=1536,
    idle_timeout=120,
    active_timeout=1800,
    accounting_mode=1,
    udps=None,
    n_dissections=20,
    statistical_analysis=False,
    splt_analysis=0,
    n_meters=0,
    performance_report=0,
)


async def capture_pcap(request):
    """Функция "capture_pcap" используется для захвата сетевого трафика и сохранения его в файл формата pcap. Она принимает запрос HTTP и считывает данные из этого запроса.

    Если данные считаны успешно, функция открывает файл "packets.pcap" в режиме добавления (append mode) с помощью модуля "aio_open" и записывает считанные данные в файл. Затем файл "packets.pcap" сохраняется, и функция возвращает HTTP-ответ с текстом "success".

    Если данные не были считаны успешно, функция возвращает HTTP-ответ с текстом "no data".
    """
    data = await request.read()
    if data:
        async with aio_open("packets.pcap", "ab") as out:
            await out.write(data)
            await out.flush()
        return web.Response(text="success")
    else:
        return web.Response(text="no data")


async def analyze_pcap():
    """Функция "analyze_pcap" предназначена для анализа файла с сетевым трафиком в формате pcap. Она работает в бесконечном цикле и каждые 30 секунд проверяет наличие файла "packets.pcap" в текущей директории. Если файл существует, он переименовывается в "to_analyze.pcap", после чего создается объект NFStreamer, который используется для анализа трафика и преобразования его в Pandas DataFrame.

    Количество потоков в объекте NFStreamer подсчитывается и выводится в лог. Затем DataFrame, полученный из объекта NFStreamer, обрабатывается с помощью функции "process_nfstream_df", после чего применяется модель машинного обучения "model" для предсказания значений. Предсказанные значения сохраняются в DataFrame в новом столбце "prediction".

    Далее данные из DataFrame сохраняются в базу данных SQL с помощью функции "to_sql", которая добавляет новые строки в таблицу "packages". В конце цикла файл "to_analyze.pcap" удаляется, и процесс начинается заново.

    Функция может выкидывать исключения, которые будут отловлены и продолжится выполнение функции. Вся информация о процессе работы функции сохраняется в лог с помощью модуля "logger".
    """
    while True:
        try:
            if os.path.exists("packets.pcap"):
                logger.info(f"packets.pcap exists. Rename to to_analyze.pcap")
                os.rename("packets.pcap", "to_analyze.pcap")
                logger.info("Creating NFStreamer")
                package_streamer = nfstream_create()
                logger.info("NFStreamer created")

                # Print the number of flows in the NFStreamer object
                flow_count = sum(1 for _ in package_streamer)
                logger.info(f"Number of flows: {flow_count}")

                logger.info("Converting streamer to pandas DataFrame")
                package_df = package_streamer.to_pandas()
                logger.info("Streamer successfully converted to DataFrame")
                logger.info("DataFrame columns:")
                logger.info(package_df.columns)
                processed_df = process_nfstream_df(package_df)
                logger.info(f"Prediction...")
                predictions = model.predict(processed_df)
                logger.info(f"Success prediction")
                package_df["prediction"] = predictions
                inserted_num = package_df.to_sql("packages", con=engine, if_exists="append")
                logger.info(f"Amount of inserted and analyzed rows to DB is {inserted_num}")
                try:
                    logger.info(f"remove to_analyse.pcap")
                    os.remove("to_analyse.pcap")
                except Exception as e:
                    logger.warning(f"error {repr(e)} while remove to_analyse.pcap")
                else:
                    logger.info(f"removed to_analyse.pcap")
        except Exception as e:
            logger.info(f"Error {e} in analyse loop. Continue...")
            continue
        await asyncio.sleep(30)


def process_nfstream_df(df):
    """Функция "process_nfstream_df" используется для обработки DataFrame, полученного из объекта NFStreamer, добавляя новые признаки и удаляя старые.

    Сначала из исходного DataFrame выбираются только необходимые признаки и переименовываются для удобства чтения. Затем вычисляются дополнительные признаки, такие как "Flow Bytes/s", "Flow Packets/s" и "Flow IAT" (время между двумя последовательными пакетами в потоке).

    Функция вычисляет среднее, максимальное и минимальное время между пакетами в потоке, а также время между первым и последним пакетом (Flow IAT Mean, Flow IAT Max, Flow IAT Min). Также вычисляются признаки для времени между пакетами только в направлении отправки (Fwd IAT) и только в направлении получения (Bwd IAT).

    Далее старые столбцы, не нужные для дальнейшего анализа, удаляются из DataFrame, пропущенные значения заполняются средними значениями по соответствующим столбцам, а бесконечные значения заменяются максимальными возможными значениями для типа данных float32.

    Функция возвращает обработанный DataFrame."""
    # Select and rename relevant features from the NFStreamer DataFrame
    processed_df = df[
        [
            "dst_port",
            "bidirectional_duration_ms",
            "src2dst_packets",
            "dst2src_packets",
            "src2dst_bytes",
            "dst2src_bytes",
            "src2dst_first_seen_ms",
            "src2dst_last_seen_ms",
            "dst2src_first_seen_ms",
            "dst2src_last_seen_ms",
        ]
    ].rename(
        columns={
            "dst_port": " Destination Port",
            "bidirectional_duration_ms": " Flow Duration",
            "src2dst_packets": " Total Fwd Packets",
            "dst2src_packets": " Total Backward Packets",
            "src2dst_bytes": "Total Length of Fwd Packets",
            "dst2src_bytes": " Total Length of Bwd Packets",
        }
    )

    # Calculate additional features
    processed_df["Flow Bytes/s"] = (
        processed_df["Total Length of Fwd Packets"] + processed_df[" Total Length of Bwd Packets"]
    ) / processed_df[" Flow Duration"]
    processed_df[" Flow Packets/s"] = (
        processed_df[" Total Fwd Packets"] + processed_df[" Total Backward Packets"]
    ) / processed_df[" Flow Duration"]

    """ 
    IAT (Interarrival Time) в датасете по интернет трафику обычно относится к временному интервалу между двумя последовательными пакетами данных, отправленными или полученными через сеть. Это может быть полезным параметром для анализа интернет-трафика, поскольку интервалы между пакетами могут раскрывать некоторые особенности в сетевой активности, такие как задержки, потери пакетов, размеры передаваемых данных, пропускную способность и т.д.

    IAT обычно измеряется в микросекундах или миллисекундах, и может использоваться для определения статистических показателей, таких как среднее время между пакетами, дисперсия времени между пакетами, медианное время между пакетами и т.д. Эти статистические показатели могут помочь в анализе производительности сети, определении возможных проблем или аномалий в сетевом трафике, а также в создании моделей прогнозирования для будущей активности сети.
    """
    processed_df[" Flow IAT Mean"] = (processed_df["src2dst_last_seen_ms"] - processed_df["src2dst_first_seen_ms"]) / (
        processed_df[" Total Fwd Packets"] - 1
    )
    processed_df[" Flow IAT Max"] = processed_df["src2dst_last_seen_ms"] - processed_df["src2dst_first_seen_ms"]
    processed_df[
        " Flow IAT Min"
    ] = 0  # максимальное время между первым и последним пакетом (выше), но минимальное вычислить не сможем точно

    # Calculate Fwd IAT features
    processed_df["Fwd IAT Total"] = processed_df["src2dst_last_seen_ms"] - processed_df["src2dst_first_seen_ms"]
    processed_df[" Fwd IAT Mean"] = processed_df[" Flow IAT Mean"]

    # Calculate Bwd IAT features
    processed_df["Bwd IAT Total"] = processed_df["dst2src_last_seen_ms"] - processed_df["dst2src_first_seen_ms"]
    processed_df[" Bwd IAT Mean"] = (processed_df["dst2src_last_seen_ms"] - processed_df["dst2src_first_seen_ms"]) / (
        processed_df[" Total Backward Packets"] - 1
    )

    # Remove old NFStreamer columns
    processed_df.drop(
        columns=["src2dst_first_seen_ms", "src2dst_last_seen_ms", "dst2src_first_seen_ms", "dst2src_last_seen_ms"],
        inplace=True,
    )

    # Fill missing values with the mean of the respective column
    processed_df.fillna(processed_df.mean(), inplace=True)

    # Replace infinity values with the maximum finite value for float32
    processed_df.replace([np.inf, -np.inf], np.finfo(np.float32).max, inplace=True)

    return processed_df


""" Код представляет собой главную программу для запуска сервера для анализа сетевого трафика.

Функция "background_tasks" создает задачу "analyse_task" с помощью функции "create_task", которая запускает функцию "analyze_pcap" для анализа сохраненного файла с сетевым трафиком. Затем функция "yield" передает управление главной программе, чтобы она могла продолжить работу. В конце функция "await" ждет завершения задачи "analyse_task".

Функция "main" создает объект приложения "app" с помощью класса "web.Application" и добавляет маршрут "/" для обработки запросов HTTP с помощью функции "capture_pcap". Затем с помощью метода "cleanup_ctx.append" добавляется функция "background_tasks" в контекст очистки (cleanup context), чтобы она запускалась при завершении работы сервера.

В конце, если этот модуль был запущен как основной (через условную конструкцию "if name == 'main':"), функция "main" запускается с помощью метода "web.run_app", который запускает сервер в бесконечном цикле до тех пор, пока его не остановят.
"""


async def background_tasks(app):
    analyse_task = asyncio.create_task(analyze_pcap())
    yield
    await analyse_task


def main():
    app = web.Application(client_max_size=None)
    app.add_routes([web.post("/", capture_pcap)])
    app.cleanup_ctx.append(background_tasks)
    web.run_app(app)


if __name__ == "__main__":
    main()
